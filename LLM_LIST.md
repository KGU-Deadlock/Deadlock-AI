## 1. 개요 (Overview)
- 'HelloCS' 서비스의 핵심 지능을 담당할 LLM(Large Language Model) 리스트
- 고성능 추론 모델과 경량화 모델을 결합한 **Hybrid LLM Architecture**를 통해 시스템 응답 속도를 극대화하고, 프로젝트 운영 비용을 최적화하는 것을 목적으로 함.

## 2. LLM 모델별 비교 분석 (Gemini vs DeepSeek vs GPT)
서비스 요구 역량에 따른 기술적 지표를 비교하여 전략적 우위를 검토함.

비교 항목 | Gemini 1.5 Flash (Main) | DeepSeek-V3 (Advanced) | GPT-4o-mini (Backup)
-- | -- | -- | --
**주요 역할** | 실시간 피드백 및 퀴즈 가이드 | 고난도 논리 추론 및 모의면접 | 텍스트 정제 및 문법 교정
**추론 엔진 효율** | 최상 (Low-Latency 특화) | 우수 (Reasoning 특화) | 양호
**Context Window** | 1.0M Tokens | 128K Tokens | 128K Tokens
**경제성 (무료 티어)** | 매우 높음 (RPM 15 지원) | 종량제 (최저가 수준) | 종량제
**특이 사항** | 긴 대화 세션 유지에 최적화 | 코딩 및 기술 지식 정확도 최상 | 범용적인 한국어 처리 안정성


## 3. 모델 선정 핵심 사유

### 3.1. 운영 비용의 획기적 절감 (Cost-Efficiency)
- **Gemini 1.5 Flash 기반 설계:** 예산이 제한된 학교 프로젝트 환경을 고려하여, 강력한 무료 할당량을 제공하는 Gemini를 메인 엔진으로 채택해 인프라 유지비를 0원에 수렴시킴.
- **RAG 대체 전략:** 100만 토큰에 달하는 거대 컨텍스트 창을 활용하여 별도의 외부 DB 구축 없이도 방대한 CS 학습 데이터를 즉시 참조하게 함으로써 시스템 복잡도를 낮춤.

### 3.2. 기술적 전문성 및 추론 깊이 확보
- **DeepSeek-V3의 전략적 배치:** 단순 질의응답을 넘어 사용자의 답변 속 오류를 정확히 파악해야 하는 모의면접 및 심화 피드백 단계에서는 고성능 추론 모델인 DeepSeek를 연동하여 서비스의 전문성을 확보함.

### 3.3. 사용자 경험 중심의 지연 시간(Latency) 최적화
- **작업별 모델 이원화:** 실시간 응답이 필수적인 인터렉션에는 처리 속도가 가장 빠른 Flash 모델을 전면에 배치하여 사용자 이탈을 방지하고 매끄러운 학습 흐름을 보장함.
